---
title: "Exploring the relationship between Diabetes and its Risk Factors using Generalized Additive Models (GAMs)"
author: "Christopher Odoom, Denis Folitse, Sandani Kumanayake, Owen Gallagher"
date: "`r Sys.Date()`"
output:
  slidy_presentation:
  beamer_presentation:
    theme: 
    colortheme: "dolphin"
    fonttheme: "structurebold"

  ioslides_presentation:
    css: 
    - styles.css
    - temp.css
  
---


```{r setup, include=FALSE, message=FALSE,warnings=FALSE}
knitr::opts_chunk$set(echo = FALSE)

  library(shiny)
    library(dplyr)
    library(ggplot2)
    library(plotly)
library(tidyverse)
library(knitr)
library(kableExtra)
library(ggpubr)
library(ggstance)
library(broom)
library(lme4)
library(MASS)
library(mgcv)
library(stargazer)
library(mice)
library(randomForest)
library(naivebayes)
library(xgboost)
library(caret)
library(magrittr)
library(ggpubr)
```



## Background of Study

- Diabetes is a chronic condition that affects how the body processes blood sugar (glucose). It occurs when the pancreas does not produce enough insulin or when the body cannot effectively use the insulin it produces.


- Type 1 diabetes, also known as juvenile diabetes, is an autoimmune disease that usually develops in childhood or adolescence. It occurs when the body's immune system mistakenly attacks and destroys the cells
in the pancreas that produce insulin.

- Type 2 diabetes, which accounts for 90-95% of all cases, is usually diagnosed in adults, but it is becoming more common in children and adolescents due to rising rates of obesity.
It occurs when the body becomes resistant to the effects of insulin or when the pancreas cannot produce enough insulin to meet the body's needs.

- The risk factors for type 2 diabetes include physical inactivity, a family history of diabetes, high blood pressure and high cholesterol.

## Motivation

- Diabetes is a leading cause of blindness, kidney failure, amputations, heart attacks, and stroke.

- According to Mayo Clinic(2021), There is no cure for diabetes, but it can be managed through lifestyle changes, such as eating a healthy diet, getting regular exercise, and monitoring blood sugar levels, as well as medications like insulin and oral hypoglycemic drugs.

- According to the World Health Organization, the number of people with diabetes has risen from 108 million in 1980 to 422 million in 2014.

- In addition to the physical health complications of diabetes, it can also have a significant impact on mental health. People with diabetes are at increased risk of depression, anxiety, and other mood disorders.

- Research is ongoing to better understand the causes of diabetes and to develop new treatments and prevention strategies. Generalized additive models (GAMs) are one tool that can be used to explore the relationship between diabetes and its risk factors.


## Objectives 


-	To understand the relationship between the risk factors of diabetes and their impact on getting diabetes using a GAM logistic regression.

-	To consider the possibility of three-way interaction using tensor product smoothing, that way we can see the interactive effects of risk factors on diabetes.

-	To compare the predictions from the GAM fit with a selected best performing machine learning algorithm for predicting diabetes.




## Methodology

**Data** 

 This data is collected from kaggle, updated by Aksha Gattatray Khare with the objective of  predicting whether a patient has diabetes,
based on certain diagnostic measurements. In total, the data contains 8 variables listed below

- ***Outcomes:*** To express the final result 1 is Yes and 0 is No

- ***Pregnancies:*** indicates the number of pregnancies

- ***Glucose:*** indicates the plasma glucose concentration

- ***Blood Pressure:***  indicates diastolic blood pressure in mm/Hg

- ***Skin Thickness:*** indicates triceps skinfold thickness in mm

- ***dpfunction:*** indicates the function which scores likelihood of diabetes based on family history

- ***age:*** indicates the age of the person

- ***Insulin:***  indicates insulin in blood (U/mL)

- ***bmi:*** indicates the body mass index in kg/m2









## Modeling Workflow 

```{r, echo=FALSE, message=FALSE,warning=FALSE,fig.height=9, fig.width=11,fig.align='center'}
library(DiagrammeR)

grViz("
  digraph flowchart {
    node [fontname = arial, shape = box, style = filled, fillcolor = lightblue, margin = 0.15, fontsize = 10]
    edge [fontname = arial, fontsize = 9]
    
    Data [label = 'Original Data Set\n768 Observations\n8 variables']
    WithoutMissingValues [label = 'MICE (Multivariate Imputation by Chained Equations)\nMethod:PMM\n(No Missing Values)']
    
    SP [label = 'Spline Model']
    ML [label = 'Machine Learning Models']
  
    TP [label = 'GAM fits and \nTensor Product (Trivariate fits)']
    RF [label = 'Random Forest (RF)']
    NB [label = 'Naive Bayes (NB)']
    SGB [label = 'Stochastic Gradient Boost\n(SGB)']
    KNN [label = 'K-Nearest Neighbors Algorithm\n(KNN)']
    
    
    Prediction [label = 'Model Prediction']
    ModelPrediction [label = 'Model Evaluation\nMetric: Misclassification']
    
    Data -> WithoutMissingValues
    WithoutMissingValues -> SP
    WithoutMissingValues -> ML
    
    SP -> TP
    
    ML -> RF
    ML -> NB
    ML -> SGB
    ML -> KNN
    
    TP -> Prediction
    RF -> Prediction
    NB -> Prediction
    SGB -> Prediction
    KNN -> Prediction
    
    Prediction -> ModelPrediction
    
  }
")

```



## Methodology

- **GAMs**

- **Tensor Product (Trivariate fits)**
  
- **Random Forest (RF)**: This is a machine-learning algorithm that builds multiple decision trees and combines them to obtain a more accurate prediction. It is used for classification and regression problems.

- **Naive Bayes (NB)**: This is a classification algorithm based on Bayes' theorem. It assumes that the predictors are independent of each other and calculates the probability of each class based on the predictor values.

- **Stochastic Gradient Boost (SGB)**: This is a machine learning algorithm that builds an ensemble of weak prediction models (e.g. decision trees) and sequentially trains them to correct the errors of the previous model. The final prediction is based on the weighted average of all the models.

- **K-Nearest Neighbors Algorithm (KNN)**: This is a non-parametric classification algorithm that assigns a new data point to the class of the majority of its k-nearest neighbors in the training set.


## Descriptive Analysis

**Histogram of Original Variables**
```{r,echo=FALSE, message=FALSE,warning=FALSE}
diabetes <- read.csv("https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv",
                     header = FALSE,
                     col.names = c("pregnancies", "glucose", "blood_pressure", "skin_thickness", "insulin", "bmi", "dpfunction", "age", "outcome"))


```




```{r,echo=FALSE, message=FALSE,warning=FALSE,fig.height=9, fig.width=11,fig.align='center'}
# Create histograms of the variables
diabetes %>% 
  dplyr::select(pregnancies, glucose, blood_pressure, skin_thickness,
                insulin, bmi, age) %>% 
  gather()%>% 
  ggplot(aes(x = value,fill=diabetes$outcome,col="blue")) + 
  facet_wrap(~key, scales = "free") +
  geom_histogram(binwidth = 5, color = "black", fill = "blue") + 
  labs(x = "", y = "Count") +
  theme_bw()


# diabetes %>% dplyr::select(insulin, outcome) %>%
#   dplyr::filter(insulin==0) %>% group_by(outcome) %>%
#   summarise(count=n()/768)

```




```{r,warning=FALSE,message=FALSE}
set.seed(123)
diabetes[, !(names(diabetes) %in% 
               c("pregnancies","dpfunction","age", "outcome"))] <- 
  lapply(diabetes[, !(names(diabetes)%in% c("pregnancies","dpfunction","age", "outcome"))], function(x) ifelse(x == 0, NA, x))


#summary(diabetes)
mids =mice(diabetes, m=5, method = c(" ","pmm","pmm",
                                     "pmm","pmm","pmm"," ", " ", " "),printFlag = FALSE)

```


## Descriptive Analysis


**Stripplot**

```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
stripplot(mids)
```







## Descriptive Analysis | continue

**Histogram of Variables**

```{r,warning=FALSE,message=FALSE}
# New Data. 


imputed_data=complete(mids)


diabetes=imputed_data

```



```{r,echo=FALSE, message=FALSE,warning=FALSE,fig.height=9, fig.width=11,fig.align='center'}
# Create histograms of the variables
diabetes %>% 
  dplyr::select(glucose, blood_pressure, skin_thickness,
                insulin, bmi) %>% 
  gather()%>% 
  ggplot(aes(x = value,fill=diabetes$outcome,col="blue")) + 
  facet_wrap(~key, scales = "free") +
  geom_histogram(binwidth = 5, color = "black", fill = "blue") + 
  labs(x = "", y = "Count") +
  theme_bw()


# diabetes %>% dplyr::select(insulin, outcome) %>%
#   dplyr::filter(insulin==0) %>% group_by(outcome) %>%
#   summarise(count=n()/768)

```






## Descriptive Analysis

**Boxplot of Variables by Outcome**

```{r,echo=FALSE, message=FALSE,warning=FALSE,fig.height=9, fig.width=11,fig.align='center'}


# Create box plots of the variables by outcome
diabetes %>% 
  gather(key = "variable", value = "value", -outcome) %>% 
  ggplot(aes(x = factor(outcome), y = value, fill = factor(outcome))) +
  facet_wrap(~variable, scales = "free") +
  geom_boxplot() +
  labs(x = "Outcome", y = "") +
  theme_bw()
```



## Descriptive Analysis

**Pair Plot**

```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
pairs(diabetes,col="blue")
```



## Descriptive Analysis
**Correlation Matrix**

```{r,echo=FALSE, message=FALSE,warning=FALSE,fig.height=9, fig.width=11,fig.align='center'}
# Create correlation matrix
corr_matrix <- cor(diabetes)
library(ggcorrplot)
# Plot correlation matrix
ggcorrplot(corr_matrix, type = "upper", hc.order = TRUE, lab = TRUE)

```





## Model Fit | GAMs splines {.smaller}

```{r,warning=FALSE,message=FALSE}
diabetes=imputed_data
setwd("/Users/sedem/Development/project/Diabetes ")

shiny::shinyAppFile("app.R")

```




```{r,echo=FALSE, message=FALSE,warning=FALSE,results = 'asis'}

### Modeling Using Gams

library(gratia)


Mod1=gam(outcome~
            s(dpfunction) +s(insulin)+s(skin_thickness)+
              s(blood_pressure)+s(pregnancies)+ te(age, glucose, bmi), 
         method = 'GCV.Cp',
         family = binomial, data = diabetes)


```





## Comparative Study




**Partial-Effect Plots**


```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
library(gratia)


draw(Mod1,select=1:5)
```




## Comparative Study




**Tensor Product Plot**


```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
library(gratia)


draw(Mod1,select=6)
```




## Comparative Study

**Appraisal**
```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
appraise(Mod1)
```







```{r,warning=FALSE,message=FALSE}
set.seed(1237)
tr.dat <- sample(nrow(diabetes), .90*nrow(diabetes), replace = FALSE)
TrainSet <- diabetes[tr.dat,]
TestSet <- diabetes[-tr.dat,]

```



```{r,warning=FALSE,message=FALSE}
Mod_gam=gam(outcome~
              s(dpfunction) + te(age, glucose, bmi)+(insulin)+s(skin_thickness)+
              s(blood_pressure)+s(pregnancies),
            method = 'GCV.Cp',
            family = binomial, data = TrainSet)

```




```{r,warning=FALSE,message=FALSE}
# summary(Mod_gam)
prediction <- ifelse(predict(Mod_gam, TestSet, type='response') > 0.5, "diabetic", "nondiabetic")

confusion  <- table(TestSet$outcome, prediction)

confusion  <- cbind(confusion, c( confusion[1,1]/(confusion[1,1]+confusion[1,2]),  confusion[2,2]/(confusion[2,1]+confusion[2,2])))


confusion  <- as.data.frame(confusion)
names(confusion) <- c('diabetic', 'nondiabetic', 'misclassification')
rownames(confusion) <- c('Non-Diabetic', 'Diabetic')


```





```{r,warning=FALSE,message=FALSE}
### Machine Learning####
TrainSet$outcome <- ifelse(TrainSet$outcome==1, "diabetic", "non_diabetic")

TestSet$outcome <- ifelse(TestSet$outcome==1, "diabetic", "non_diabetic")

#Checking the proportions of the splitted data
#The splitting proportion is 80/20 percent
# prop.table(table(diabetes$outcome))
# prop.table(table(TrainSet$outcome))
# prop.table(table(TestSet$outcome))
```




```{r,warning=FALSE,message=FALSE}
#Tuning Parameters

fitControl <- trainControl(method = "repeatedcv", number = 10,
                           repeats = 10, classProbs = TRUE,
                           summaryFunction = twoClassSummary,
                           search = "grid", 
                           preProcOptions = c("center", "scale"))

```



## Random Forest Output

```{r,warning=FALSE,message=FALSE,fig.height=9, fig.width=11,fig.align='center'}
#Random Forest Model
randf <- caret::train(outcome ~ ., data = TrainSet, method = "rf",
                      trControl = fitControl, metric = "ROC")



plot(varImp(randf))



```


```{r,warning=FALSE,message=FALSE}
#Predicting
pred.rf <- predict(randf,TestSet)  

#levels(pred.rf)[1] <- "Diabetes"
tab.rf <-table(factor(TestSet$outcome), pred.rf)
tab.rf<- cbind(tab.rf, c( tab.rf[1,2]/(tab.rf[1,1]+tab.rf[1,2]), 
                          tab.rf[2,1]/(tab.rf[2,1]+tab.rf[2,2])))
colnames(tab.rf) <- c('diabetic', 'non_diabetic', 'misclassification')


```




```{r,warning=FALSE,message=FALSE}
#Naive Bayes
nav <- caret::train(as.factor(outcome) ~., data = TrainSet, method ="nb"  , 
                    trControl = fitControl, metric = "ROC")

#nav
#plot(varImp(nav))
#Predicting
pred.nav <- predict(nav,TestSet)  


tab.nav<-table(factor(TestSet$outcome), pred.nav)
tab.nav<- cbind(tab.nav, c( tab.nav[1,2]/(tab.nav[1,1]+tab.nav[1,2]), 
                            tab.nav[2,1]/(tab.nav[2,1]+tab.nav[2,2])))
colnames(tab.nav) <- c('diabetic', 'non_diabetic', 'misclassification')



```


```{r,warning=FALSE,message=FALSE}
#Stochastic Gradient Boost
sgb <- train(outcome ~., data = TrainSet, method = "gbm", 
             metric = "ROC", trControl = fitControl,verbose = FALSE)
#sgb
#Predicting
pred.sgb <- predict(sgb,TestSet)  


tab.sgb<-table(factor(TestSet$outcome), pred.sgb)
tab.sgb<- cbind(tab.sgb, c( tab.sgb[1,2]/(tab.sgb[1,1]+tab.sgb[1,2]), 
                            tab.sgb[2,1]/(tab.sgb[2,1]+tab.sgb[2,2])))
colnames(tab.sgb) <- c('diabetic', 'non_diabetic', 'misclassification')



```





```{r,warning=FALSE,message=FALSE}

#K Nearest Neighbor
knn <- train(outcome ~., data = TrainSet, method = "knn", 
             metric = "ROC", trControl = fitControl)
#Predicting
pred.knn <- predict(knn,TestSet)  

#levels(pred.rf)[1] <- "Diabetes"
tab.knn <-table(factor(TestSet$outcome), pred.knn)
tab.knn<- cbind(tab.knn, c( tab.knn[1,2]/(tab.knn[1,1]+tab.knn[1,2]), 
                            tab.knn[2,1]/(tab.knn[2,1]+tab.knn[2,2])))
colnames(tab.knn) <- c('diabetic', 'non_diabetic', 'misclassification')

rownames(tab.knn) <- c('diabetic_KNN', 'non_diabetic_KNN')

```


## Comparative Study 


```{r,warning=FALSE,message=FALSE}


tabb=data.frame(Model=c("GAMs","NB","RF","SGB","KNN"),Diabetic_Misclass=c(confusion[2,3],tab.nav[1,3],tab.rf[1,3],tab.sgb[1,3],tab.knn[1,3]),Non_Diabetic_Misclass=c(confusion[1,3],tab.nav[2,3],tab.rf[2,3],tab.sgb[2,3],tab.knn[2,3]))

kable(tabb, table.attr = 'style="width:80%; height:500px; overflow-y:scroll;"')


```



## Further Studies

- Study the influence of different methods of Multiple Imputations by chained equations (mice) on the GAMs result.


- Pool results from GAM fits on multiple mice-imputed dataset 




## References

- American Diabetes Association. (n.d.). What is Diabetes? https://www.diabetes.org/diabetes/what-is-diabetes

- World Health Organization. (2016). Global report on diabetes. https://www.who.int/publications/i/item/9789241565257

- American Diabetes Association. (n.d.). Diabetes Complications. https://www.diabetes.org/diabetes/complications

- Mayo Clinic. (2021). Type 1 diabetes. https://www.mayoclinic.org/diseases-conditions/type-1-diabetes/symptoms-causes/syc-20353011

- Centers for Disease Control and Prevention. (2021). Type 2 Diabetes. https://www.cdc.gov/diabetes/basics/type2.html

- Mayo Clinic. (2021). Diabetes.
https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451

- Centers for Disease Control and Prevention. (2021). Risk Factors for Type 2 Diabetes. https://www.cdc.gov/diabetes/basics/risk-factors.html

- American Diabetes Association. (n.d.). Mental Health. https://www.diabetes.org/diabetes/mental-health

- Wood, S. N. (2017). Generalized Additive Models: An Introduction with R (2nd ed.). CRC Press.
